# ASL Alphabet Recognition with MediaPipe

This repository captures ASL handshapes via webcam, collects landmark data, trains a TensorFlow classifier, and runs real-time inference using MediaPipe.

```
📁 asl_recognition/
├── data_collection.py
├── train_model.py
├── inference.py
├── requirements.txt
└── README.md
```

---

## requirements.txt
```text
mediapipe==0.10.1
opencv-python
tensorflow
pandas
scikit-learn
numpy
```

---

## data_collection.py
```python
import cv2
import mediapipe as mp
import numpy as np
import os
import csv

# Settings
dest_folder = 'data'
labels = [
    'A','B','C','D','E','F','G','H','I','J',
    'K','L','M','N','O','P','Q','R','S','T',
    'U','V','W','X','Y','Z'
]
num_samples_per_label = 200

# Setup MediaPipe
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=1,
    min_detection_confidence=0.5
)
mp_drawing = mp.solutions.drawing_utils

# Create data folder and CSV
os.makedirs(dest_folder, exist_ok=True)
csv_path = os.path.join(dest_folder, 'landmarks.csv')
with open(csv_path, 'w', newline='') as f:
    writer = csv.writer(f)
    writer.writerow(['label'] + [f'x{i}' for i in range(21)] + [f'y{i}' for i in range(21)])

# Start video capture
cap = cv2.VideoCapture(0)
for label in labels:
    print(f"Collecting data for label '{label}'")
    count = 0
    while count < num_samples_per_label:
        ret, frame = cap.read()
        if not ret:
            continue
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = hands.process(frame_rgb)
        if results.multi_hand_landmarks:
            lm = results.multi_hand_landmarks[0]
            coords = []
            for pt in lm.landmark:
                coords.extend([pt.x, pt.y])
            with open(csv_path, 'a', newline='') as f:
                writer = csv.writer(f)
                writer.writerow([label] + coords)
            count += 1
            mp_drawing.draw_landmarks(frame, lm, mp_hands.HAND_CONNECTIONS)
        cv2.putText(
            frame,
            f"{label}: {count}/{num_samples_per_label}",
            (10, 30),
            cv2.FONT_HERSHEY_SIMPLEX,
            1,
            (0, 255, 0),
            2
        )
        cv2.imshow('Data Collection', frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
cap.release()
cv2.destroyAllWindows()
```

---

## train_model.py
```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
from tensorflow.keras import layers, models
import os

# Load data
df = pd.read_csv('data/landmarks.csv')
X = df.drop('label', axis=1).values
y = LabelEncoder().fit_transform(df['label'])

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Build model
model = models.Sequential([
    layers.Input(shape=(42,)),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(64, activation='relu'),
    layers.Dense(len(np.unique(y)), activation='softmax')
])
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train model
model.fit(
    X_train,
    y_train,
    epochs=30,
    batch_size=32,
    validation_data=(X_test, y_test)
)

# Save model
os.makedirs('hand_model', exist_ok=True)
model.save('hand_model/keras_model.h5')

# Convert to TFLite
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
with open('hand_model/model.tflite', 'wb') as f:
    f.write(tflite_model)
```

---

## inference.py
```python
import cv2
import mediapipe as mp
import numpy as np
import tensorflow as tf
import pandas as pd

# Load TFLite model
interpreter = tf.lite.Interpreter(model_path='hand_model/model.tflite')
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Load labels
df = pd.read_csv('data/landmarks.csv')
labels = sorted(df['label'].unique())

# Setup MediaPipe
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(
    min_detection_confidence=0.5,
    max_num_hands=1
)
mp_drawing = mp.solutions.drawing_utils

# Start video capture
cap = cv2.VideoCapture(0)
while True:
    ret, frame = cap.read()
    if not ret:
        continue
    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = hands.process(img_rgb)
    if results.multi_hand_landmarks:
        lm = results.multi_hand_landmarks[0]
        coords = np.array(
            [[pt.x, pt.y] for pt in lm.landmark]
        ).flatten().astype(np.float32)
        interpreter.set_tensor(input_details[0]['index'], [coords])
        interpreter.invoke()
        preds = interpreter.get_tensor(output_details[0]['index'])[0]
        pred_label = labels[np.argmax(preds)]
        cv2.putText(
            frame,
            pred_label,
            (10, 60),
            cv2.FONT_HERSHEY_SIMPLEX,
            2,
            (255, 0, 0),
            3
        )
        mp_drawing.draw_landmarks(frame, lm, mp_hands.HAND_CONNECTIONS)
    cv2.imshow('ASL Recognition', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
cap.release()
cv2.destroyAllWindows()
```

---

## README.md
```markdown
# ASL Alphabet Recognition with MediaPipe

This project captures ASL handshapes (A–Z), trains a classifier, and runs real-time detection via webcam.

## Setup

```bash
git clone https://github.com/<your-username>/asl_recognition.git
cd asl_recognition
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

## Usage

1. **Collect data**  
   `python data_collection.py`

2. **Train model**  
   `python train_model.py`

3. **Run inference**  
   `python inference.py`

Press `q` in any window to quit.

---

All labels (A–Z) and paths are preconfigured—just copy, paste, and run!
```
