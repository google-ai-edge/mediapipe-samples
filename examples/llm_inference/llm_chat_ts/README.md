# MediaPipe LLM Inference Chat Demo

This is a web-based chat demo that uses the MediaPipe LLM Inference API to run a large language model entirely in the browser.

## Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/google-ai-edge/mediapipe-samples.git
    ```
2.  **Navigate to the demo directory:**
    ```bash
    cd mediapipe-samples/examples/llm_inference/llm_chat
    ```
3.  **Install the dependencies:**
    ```bash
    npm install
    ```

## Running the Demo

1.  **Run the development server:**
    ```bash
    npm run dev
    ```
    This will start a local development server and open the demo in your browser. When you first run the demo, you will be prompted to sign in with your Hugging Face account to access the models.
