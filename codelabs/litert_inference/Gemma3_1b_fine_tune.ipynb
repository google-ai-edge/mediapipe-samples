{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df840597-64ce-4834-852e-48ced451f69f"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gemma-3-1B fine-tuning with SFT and on-device deployment with AI edge torch and MediaPipe."
      ],
      "metadata": {
        "id": "39AMoCOa1ckc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this colab, we will show you how to fine-tune a Gemma3-1B model using a synthetic reasoning dataset, finetune the model with LoRA adaptors and then convert the model to LiteRT format. Lastly we will load the LiteRT model and perform some inferences in colab environment.\n",
        "\n",
        "(Note: to run this colab smoothly you will need a Colab Pro subscription which gives you GPU and high RAM access)."
      ],
      "metadata": {
        "id": "YvEKVMFw3rlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prerequisite"
      ],
      "metadata": {
        "id": "qgwkcOcjGKEM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Create HuggingFace token with permission access to\n",
        "  - google/gemma-3-1b\n",
        "\n",
        "  This is needed to download the tflite model and tokenizer.\n",
        "\n",
        "- Open Colab Secrets: In your Google Colab notebook, locate the Secrets icon in the left-hand sidebar and click on it.\n",
        "- Add a new secret: Click the \"Add Secret\" button.\n",
        "- Name your secret: Enter \"HF_TOKEN\" for your token in the \"Name\" field.\n",
        "- Paste your token: In the \"Value\" field, paste the actual token you want to store."
      ],
      "metadata": {
        "id": "868qAg3KGNVp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9bdc007e6ce"
      },
      "source": [
        "Note: When running notebooks in this repository with Google Colab, some users may see\n",
        "the following warning message:\n",
        "\n",
        "![Colab warning](https://github.com/google-ai-edge/ai-edge-torch/blob/main/docs/data/colab_warning.jpg?raw=true)\n",
        "\n",
        "Please click `Restart Session` and run again."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install dependencies"
      ],
      "metadata": {
        "id": "A-ppVrWQTGpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install --upgrade -q -U bitsandbytes==0.46.0\n",
        "!pip3 install --upgrade -q -U peft==0.15.2\n",
        "!pip3 install --upgrade -q -U trl==0.18.1\n",
        "!pip3 install --upgrade -q -U accelerate==1.7.0\n",
        "!pip3 install --upgrade -q -U datasets==3.6.0\n",
        "!pip3 install --upgrade -q -U numpy==2.2.6\n",
        "!pip3 install --force-reinstall transformers==4.52.3"
      ],
      "metadata": {
        "id": "WQUK_YNLZPnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install ai-edge-torch-nightly==0.6.0.dev20250605\n",
        "! pip3 install ai-edge-litert==1.3.0\n",
        "! pip3 install mediapipe==0.10.21"
      ],
      "metadata": {
        "id": "_2zWjAjDXHs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "I36jgF9oZx0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download Gemma-3-1B from HuggingFace and set up tokenizer."
      ],
      "metadata": {
        "id": "u55--scLTpl_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4ypIkm0RRVN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, BitsAndBytesConfig, GemmaTokenizer\n",
        "from transformers.models.gemma3 import Gemma3ForCausalLM\n",
        "\n",
        "model_id = 'google/gemma-3-1b-pt'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=os.environ['HF_TOKEN'])\n",
        "model = Gemma3ForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token=os.environ['HF_TOKEN'], attn_implementation='eager')\n",
        "# Set up the chat format\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.chat_template = \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, with a simple prompt (\"What is the primary function of mitochondria within a cell?\"), from the sample output we can see that the base model is repeating user questions (which is expected before the fine-tuning step)."
      ],
      "metadata": {
        "id": "Qq1W60LBVekL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Let's test the base model before training\n",
        "prompt = \"What is the primary function of mitochondria within a cell?\"\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "pipe(prompt, max_new_tokens=100)"
      ],
      "metadata": {
        "id": "kCa1726Tahgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set up LoRA configurations, datasets and SFT training procedure.\n",
        "\n"
      ],
      "metadata": {
        "id": "53x-N_nbWGY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "from peft import LoraConfig, PeftModel\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ],
      "metadata": {
        "id": "3sp2nAMsScYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the SFT reasoning dataset from HuggingFace(argilla/synthetic-concise-reasoning-sft-filtered)."
      ],
      "metadata": {
        "id": "x1BxX6R9WvIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"argilla/synthetic-concise-reasoning-sft-filtered\")\n",
        "def tokenize_function(examples):\n",
        "    # Process all examples in the batch\n",
        "    prompts = examples[\"prompt\"]\n",
        "    completions = examples[\"completion\"]\n",
        "    texts = []\n",
        "    for prompt, completion in zip(prompts, completions):\n",
        "        text = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt.strip()}, {\"role\": \"assistant\", \"content\": completion.strip()}], tokenize=False)\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts }  # Return a list of texts\n",
        "\n",
        "ds = ds.map(tokenize_function, batched = True)"
      ],
      "metadata": {
        "id": "HAOZk4ZehEg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start the fine-tuning with 150 training steps (which will take ~3 minutes with single A100). Alternatively you can set `num_train_epochs=1` if you want to train with the entire SFT dataset, that will lead to even longer training times"
      ],
      "metadata": {
        "id": "CWC9WYSAW5CV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    train_dataset = ds['train'],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=2,\n",
        "        max_steps=150,\n",
        "        #num_train_epochs=1,\n",
        "        # Copied from other hugging face tuning blog posts\n",
        "        learning_rate=2e-4,\n",
        "        #fp16=True,\n",
        "        bf16=True,\n",
        "        # It makes training faster\n",
        "        logging_steps=1,\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        "    peft_config=lora_config,\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "e0L_QYka9NDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's save the trainer weights, and run a few inference steps on the fine-tuned model to make sure it can perform question answering. Weights will be saved in a folder named \"gemma3-1b-sft\"."
      ],
      "metadata": {
        "id": "_Yug8eWL0Bv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"gemma3-1b-sft\")"
      ],
      "metadata": {
        "id": "EtXWqjITqzcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "# Let's test the base model before training\n",
        "prompt = \"What is the primary function of mitochondria within a cell?\"\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "pipe(prompt, max_new_tokens=100)"
      ],
      "metadata": {
        "id": "zolrP9XXDDVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can merge the LoRA weights to the base model, and the saved checkpoint will be imported with ai-edge-torch to create a LiteRT model for on-device inference. Merged weights will be saved in a folder named \"merged_model\"."
      ],
      "metadata": {
        "id": "HOF8njAI0XM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import AutoPeftModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Load PEFT model on CPU\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\"gemma3-1b-sft\")\n",
        "# Merge LoRA and base model and save\n",
        "merged_model = model.merge_and_unload()\n",
        "# Resize vocab size to match with base model vocabulary table.\n",
        "merged_model.resize_token_embeddings(262144)\n",
        "merged_model.save_pretrained(\"merged_model\", safe_serialization=True, max_shard_size=\"2GB\")"
      ],
      "metadata": {
        "id": "e8hzAdwdrG5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run inference again on the merged model to ensure if works as expected."
      ],
      "metadata": {
        "id": "Tv7v1eMs0wic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "prompt = \"What is the primary function of mitochondria within a cell?\"\n",
        "pipe = pipeline(\"text-generation\", model=merged_model, tokenizer=tokenizer)\n",
        "pipe(prompt, max_new_tokens=100)"
      ],
      "metadata": {
        "id": "fyrx1uiprWbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load up the checkpoint in AI edge torch and convert to LiteRT."
      ],
      "metadata": {
        "id": "9eZ42Mo40_6s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's convert our model(including 8-bit quantization) to LiteRT format, this will take roughly 10+ minutes to finish. The output tflite will be saved in the \"/content\" subfolder, with the name \"gemma3_1b_finetune_q8_ekv1024.tflite\""
      ],
      "metadata": {
        "id": "r3XJyw083lnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from ai_edge_torch.generative.examples.gemma3 import gemma3\n",
        "from ai_edge_torch.generative.layers import kv_cache\n",
        "from ai_edge_torch.generative.utilities import converter\n",
        "from ai_edge_torch.generative.utilities.export_config import ExportConfig\n",
        "\n",
        "\n",
        "PREFILL_SEQ_LENS = [128]\n",
        "KV_CACHE_MAX_LEN = 1024\n",
        "\n",
        "def _create_mask(mask_len, kv_cache_max_len):\n",
        "  mask = torch.full(\n",
        "      (mask_len, kv_cache_max_len), float('-inf'), dtype=torch.float32\n",
        "  )\n",
        "  mask = torch.triu(mask, diagonal=1).unsqueeze(0).unsqueeze(0)\n",
        "  return mask\n",
        "\n",
        "\n",
        "def _create_export_config(\n",
        "    prefill_seq_lens: list[int], kv_cache_max_len: int\n",
        ") -> ExportConfig:\n",
        "  \"\"\"Creates the export config for the model.\"\"\"\n",
        "  export_config = ExportConfig()\n",
        "  if isinstance(prefill_seq_lens, list):\n",
        "    prefill_mask = [_create_mask(i, kv_cache_max_len) for i in prefill_seq_lens]\n",
        "  else:\n",
        "    prefill_mask = _create_mask(prefill_seq_lens, kv_cache_max_len)\n",
        "\n",
        "  export_config.prefill_mask = prefill_mask\n",
        "\n",
        "  decode_mask = torch.full(\n",
        "      (1, kv_cache_max_len), float('-inf'), dtype=torch.float32\n",
        "  )\n",
        "  decode_mask = torch.triu(decode_mask, diagonal=1).unsqueeze(0).unsqueeze(0)\n",
        "  export_config.decode_mask = decode_mask\n",
        "  export_config.kvcache_layout = kv_cache.KV_LAYOUT_TRANSPOSED\n",
        "  export_config.mask_as_input = True\n",
        "  return export_config\n",
        "\n",
        "\n",
        "def convert_to_litert():\n",
        "  with torch.inference_mode(True):\n",
        "    pytorch_model = gemma3.build_model_1b(\n",
        "      \"/content/merged_model\", mask_cache_size=KV_CACHE_MAX_LEN,\n",
        "    )\n",
        "    converter.convert_to_tflite(\n",
        "        pytorch_model,\n",
        "        output_path=\"/content/\",\n",
        "        output_name_prefix=\"gemma3_1b_finetune\",\n",
        "        prefill_seq_len=PREFILL_SEQ_LENS,\n",
        "        kv_cache_max_len=KV_CACHE_MAX_LEN,\n",
        "        quantize=converter.QuantizationName.DYNAMIC_INT4_BLOCK32,\n",
        "        lora_ranks=None,\n",
        "        export_config=_create_export_config(\n",
        "            prefill_seq_lens=PREFILL_SEQ_LENS,\n",
        "            kv_cache_max_len=KV_CACHE_MAX_LEN,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "# Run model conversion.\n",
        "convert_to_litert()"
      ],
      "metadata": {
        "id": "xTCQEekaXoZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ai_edge_litert import interpreter as interpreter_lib\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "from collections.abc import Sequence\n",
        "import sys"
      ],
      "metadata": {
        "id": "mB7KUJSjkeD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_id = 'google/gemma-3-1b-pt'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.chat_template = \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\""
      ],
      "metadata": {
        "id": "VlIm1k88kvL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = interpreter_lib.InterpreterWithCustomOps(\n",
        "    custom_op_registerers=[\"pywrap_genai_ops.GenAIOpsRegisterer\"],\n",
        "    model_path=\"/content/gemma3_1b_finetune_q4_block32_ekv1024.tflite\",\n",
        "    num_threads=2,\n",
        "    experimental_default_delegate_latest_features=True)"
      ],
      "metadata": {
        "id": "KQUY3hMDk4km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create pipeline with LiteRT models"
      ],
      "metadata": {
        "id": "AM6rDABTXt2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_mask(shape: Sequence[int], k: int):\n",
        "  \"\"\"Gets the mask for the input to the model.\n",
        "\n",
        "  Args:\n",
        "    shape: The shape of the mask input to the model.\n",
        "    k: all elements below the k-th diagonal are set to 0.\n",
        "\n",
        "  Returns:\n",
        "    The mask for the input to the model. All the elements in the mask are set\n",
        "    to -inf except that all the elements below the k-th diagonal are set to 0.\n",
        "  \"\"\"\n",
        "  mask = np.ones(shape, dtype=np.float32) * float(\"-inf\")\n",
        "  mask = np.triu(mask, k=k)\n",
        "  return mask\n",
        "\n",
        "class LiteRTLlmPipeline:\n",
        "\n",
        "  def __init__(self, interpreter, tokenizer):\n",
        "    \"\"\"Initializes the pipeline.\"\"\"\n",
        "    self._interpreter = interpreter\n",
        "    self._tokenizer = tokenizer\n",
        "\n",
        "    self._prefill_runner = None\n",
        "    self._decode_runner = self._interpreter.get_signature_runner(\"decode\")\n",
        "\n",
        "\n",
        "  def _init_prefill_runner(self, num_input_tokens: int):\n",
        "    \"\"\"Initializes all the variables related to the prefill runner.\n",
        "\n",
        "    This method initializes the following variables:\n",
        "      - self._prefill_runner: The prefill runner based on the input size.\n",
        "      - self._max_seq_len: The maximum sequence length supported by the model.\n",
        "\n",
        "    Args:\n",
        "      num_input_tokens: The number of input tokens.\n",
        "    \"\"\"\n",
        "    if not self._interpreter:\n",
        "      raise ValueError(\"Interpreter is not initialized.\")\n",
        "\n",
        "    # Prefill runner related variables will be initialized in `predict_text` and\n",
        "    # `compute_log_likelihood`.\n",
        "    self._prefill_runner = self._get_prefill_runner(num_input_tokens)\n",
        "    # input_token_shape has shape (batch, max_seq_len)\n",
        "    input_token_shape = self._prefill_runner.get_input_details()[\"tokens\"][\n",
        "        \"shape\"\n",
        "    ]\n",
        "    if len(input_token_shape) == 1:\n",
        "      self._max_seq_len = input_token_shape[0]\n",
        "    else:\n",
        "      self._max_seq_len = input_token_shape[1]\n",
        "\n",
        "    # kv cache input has shape [batch=1, num_kv_heads, cache_size, head_dim].\n",
        "    kv_cache_shape = self._prefill_runner.get_input_details()[\"kv_cache_k_0\"][\n",
        "        \"shape\"\n",
        "    ]\n",
        "    self._max_kv_cache_seq_len = kv_cache_shape[2]\n",
        "\n",
        "  def _init_kv_cache(self) -> dict[str, np.ndarray]:\n",
        "    if self._prefill_runner is None:\n",
        "      raise ValueError(\"Prefill runner is not initialized.\")\n",
        "    kv_cache = {}\n",
        "    for input_key in self._prefill_runner.get_input_details().keys():\n",
        "      if \"kv_cache\" in input_key:\n",
        "        kv_cache[input_key] = np.zeros(\n",
        "            self._prefill_runner.get_input_details()[input_key][\"shape\"],\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "        kv_cache[input_key] = np.zeros(\n",
        "            self._prefill_runner.get_input_details()[input_key][\"shape\"],\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "    return kv_cache\n",
        "\n",
        "  def _get_prefill_runner(self, num_input_tokens: int) :\n",
        "    \"\"\"Gets the prefill runner with the best suitable input size.\n",
        "\n",
        "    Args:\n",
        "      num_input_tokens: The number of input tokens.\n",
        "\n",
        "    Returns:\n",
        "      The prefill runner with the smallest input size.\n",
        "    \"\"\"\n",
        "    best_signature = None\n",
        "    delta = sys.maxsize\n",
        "    max_prefill_len = -1\n",
        "    for key in self._interpreter.get_signature_list().keys():\n",
        "      if \"prefill\" not in key:\n",
        "        continue\n",
        "      input_pos = self._interpreter.get_signature_runner(key).get_input_details()[\n",
        "          \"input_pos\"\n",
        "      ]\n",
        "      # input_pos[\"shape\"] has shape (max_seq_len, )\n",
        "      seq_size = input_pos[\"shape\"][0]\n",
        "      max_prefill_len = max(max_prefill_len, seq_size)\n",
        "      if num_input_tokens <= seq_size and seq_size - num_input_tokens < delta:\n",
        "        delta = seq_size - num_input_tokens\n",
        "        best_signature = key\n",
        "    if best_signature is None:\n",
        "      raise ValueError(\n",
        "          \"The largest prefill length supported is %d, but we have %d number of input tokens\"\n",
        "          %(max_prefill_len, num_input_tokens)\n",
        "      )\n",
        "    return self._interpreter.get_signature_runner(best_signature)\n",
        "\n",
        "  def _run_prefill(\n",
        "      self, prefill_token_ids: Sequence[int],\n",
        "  ) -> dict[str, np.ndarray]:\n",
        "    \"\"\"Runs prefill and returns the kv cache.\n",
        "\n",
        "    Args:\n",
        "      prefill_token_ids: The token ids of the prefill input.\n",
        "\n",
        "    Returns:\n",
        "      The updated kv cache.\n",
        "    \"\"\"\n",
        "    if not self._prefill_runner:\n",
        "      raise ValueError(\"Prefill runner is not initialized.\")\n",
        "    prefill_token_length = len(prefill_token_ids)\n",
        "    if prefill_token_length == 0:\n",
        "      return self._init_kv_cache()\n",
        "\n",
        "    # Prepare the input to be [1, max_seq_len].\n",
        "    input_token_ids = [0] * self._max_seq_len\n",
        "    input_token_ids[:prefill_token_length] = prefill_token_ids\n",
        "    input_token_ids = np.asarray(input_token_ids, dtype=np.int32)\n",
        "    input_token_ids = np.expand_dims(input_token_ids, axis=0)\n",
        "\n",
        "    # Prepare the input position to be [max_seq_len].\n",
        "    input_pos = [0] * self._max_seq_len\n",
        "    input_pos[:prefill_token_length] = range(prefill_token_length)\n",
        "    input_pos = np.asarray(input_pos, dtype=np.int32)\n",
        "\n",
        "    # Initialize kv cache.\n",
        "    prefill_inputs = self._init_kv_cache()\n",
        "    # Prepare the tokens and input position inputs.\n",
        "    prefill_inputs.update({\n",
        "        \"tokens\": input_token_ids,\n",
        "        \"input_pos\": input_pos,\n",
        "    })\n",
        "    if \"mask\" in self._prefill_runner.get_input_details().keys():\n",
        "      # For prefill, mask has shape [batch=1, 1, seq_len, kv_cache_size].\n",
        "      # We want mask[0, 0, i, j] = 0 for j<=i and -inf otherwise.\n",
        "      prefill_inputs[\"mask\"] = _get_mask(\n",
        "          shape=self._prefill_runner.get_input_details()[\"mask\"][\"shape\"],\n",
        "          k=1,\n",
        "      )\n",
        "    prefill_outputs = self._prefill_runner(**prefill_inputs)\n",
        "    if \"logits\" in prefill_outputs:\n",
        "      # Prefill outputs includes logits and kv cache. We only output kv cache.\n",
        "      prefill_outputs.pop(\"logits\")\n",
        "\n",
        "    return prefill_outputs\n",
        "\n",
        "  def _greedy_sampler(self, logits: np.ndarray) -> int:\n",
        "    return int(np.argmax(logits))\n",
        "\n",
        "\n",
        "  def _run_decode(\n",
        "      self,\n",
        "      start_pos: int,\n",
        "      start_token_id: int,\n",
        "      kv_cache: dict[str, np.ndarray],\n",
        "      max_decode_steps: int,\n",
        "  ) -> str:\n",
        "    \"\"\"Runs decode and outputs the token ids from greedy sampler.\n",
        "\n",
        "    Args:\n",
        "      start_pos: The position of the first token of the decode input.\n",
        "      start_token_id: The token id of the first token of the decode input.\n",
        "      kv_cache: The kv cache from the prefill.\n",
        "      max_decode_steps: The max decode steps.\n",
        "\n",
        "    Returns:\n",
        "      The token ids from the greedy sampler.\n",
        "    \"\"\"\n",
        "    next_pos = start_pos\n",
        "    next_token = start_token_id\n",
        "    decode_text = []\n",
        "    decode_inputs = kv_cache\n",
        "\n",
        "    for _ in range(max_decode_steps):\n",
        "      decode_inputs.update({\n",
        "          \"tokens\": np.array([[next_token]], dtype=np.int32),\n",
        "          \"input_pos\": np.array([next_pos], dtype=np.int32),\n",
        "      })\n",
        "      if \"mask\" in self._decode_runner.get_input_details().keys():\n",
        "        # For decode, mask has shape [batch=1, 1, 1, kv_cache_size].\n",
        "        # We want mask[0, 0, 0, j] = 0 for j<=next_pos and -inf otherwise.\n",
        "        decode_inputs[\"mask\"] = _get_mask(\n",
        "            shape=self._decode_runner.get_input_details()[\"mask\"][\"shape\"],\n",
        "            k=next_pos + 1,\n",
        "        )\n",
        "      decode_outputs = self._decode_runner(**decode_inputs)\n",
        "      # Output logits has shape (batch=1, 1, vocab_size). We only take the first\n",
        "      # element.\n",
        "      logits = decode_outputs.pop(\"logits\")[0][0]\n",
        "      next_token = self._greedy_sampler(logits)\n",
        "      if next_token == self._tokenizer.eos_token_id:\n",
        "        break\n",
        "      decode_text.append(self._tokenizer.decode(next_token, skip_special_tokens=True))\n",
        "      if len(decode_text[-1]) == 0:\n",
        "        # Break out the loop if we hit the special token.\n",
        "        break\n",
        "\n",
        "      print(decode_text[-1], end='', flush=True)\n",
        "      # Decode outputs includes logits and kv cache. We already poped out\n",
        "      # logits, so the rest is kv cache. We pass the updated kv cache as input\n",
        "      # to the next decode step.\n",
        "      decode_inputs = decode_outputs\n",
        "      next_pos += 1\n",
        "\n",
        "    print() # print a new line at the end.\n",
        "    return ''.join(decode_text)\n",
        "\n",
        "  def generate(self, prompt: str, max_decode_steps: int | None = None) -> str:\n",
        "    messages=[{ 'role': 'user', 'content': prompt}]\n",
        "    token_ids = self._tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)\n",
        "    # Initialize the prefill runner with the suitable input size.\n",
        "    self._init_prefill_runner(len(token_ids))\n",
        "\n",
        "    # Run prefill.\n",
        "    # Prefill up to the seond to the last token of the prompt, because the last\n",
        "    # token of the prompt will be used to bootstrap decode.\n",
        "    prefill_token_length = len(token_ids) - 1\n",
        "\n",
        "    print('Running prefill')\n",
        "    kv_cache = self._run_prefill(token_ids[:prefill_token_length])\n",
        "    # Run decode.\n",
        "    print('Running decode')\n",
        "    actual_max_decode_steps = self._max_kv_cache_seq_len - prefill_token_length - 1\n",
        "    if max_decode_steps is not None:\n",
        "      actual_max_decode_steps = min(actual_max_decode_steps, max_decode_steps)\n",
        "    decode_text = self._run_decode(\n",
        "        prefill_token_length,\n",
        "        token_ids[prefill_token_length],\n",
        "        kv_cache,\n",
        "        actual_max_decode_steps,\n",
        "    )\n",
        "    return decode_text"
      ],
      "metadata": {
        "id": "UBSGrHrM4ANm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate text from model"
      ],
      "metadata": {
        "id": "dASKx_JtYXwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Disclaimer: Model performance demonstrated with the Python API in this notebook is not representative of performance on a local device.\n",
        "pipeline = LiteRTLlmPipeline(interpreter, tokenizer)"
      ],
      "metadata": {
        "id": "AZhlDQWg61AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is the primary function of mitochondria within a cell\"\n",
        "output = pipeline.generate(prompt, max_decode_steps = 100)"
      ],
      "metadata": {
        "id": "wT9BIiATkjzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare task bundle for MediaPipe deployment"
      ],
      "metadata": {
        "id": "WKCkENPQk-Gg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The task file will be named as \"gemma3_1b_it_q8_ekv1280.task\", and placed under the \"/content\" directory. Please refer to https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference about how to deploy the `task` file with MediaPipe LLM inference example."
      ],
      "metadata": {
        "id": "rD7ZG0lsfQPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import hf_hub_download\n",
        "import joblib\n",
        "\n",
        "REPO_ID = \"google/gemma-3-1b-it\"\n",
        "FILENAME = \"tokenizer.model\"\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "\n",
        "tokenizer_model = (\n",
        "    hf_hub_download(repo_id=REPO_ID, filename=FILENAME, local_dir=\"/content\", token=os.environ['HF_TOKEN'])\n",
        ")"
      ],
      "metadata": {
        "id": "CC7AnELBmYwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mediapipe.tasks.python.genai.bundler import llm_bundler\n",
        "\n",
        "def build_gemma3_1b_it_block_q4():\n",
        "  output_file = \"/content/gemma3_1b_finetune_q4_block32_ekv1024.task\"\n",
        "  tflite_model = \"/content/gemma3_1b_finetune_q4_block32_ekv1024.tflite\"\n",
        "  tokenizer_model = (\n",
        "      \"/content/tokenizer.model\"\n",
        "  )\n",
        "  config = llm_bundler.BundleConfig(\n",
        "      tflite_model=tflite_model,\n",
        "      tokenizer_model=tokenizer_model,\n",
        "      start_token=\"<bos>\",\n",
        "      stop_tokens=[\"<eos>\"],\n",
        "      output_filename=output_file,\n",
        "      enable_bytes_to_unicode_mapping=False,\n",
        "      prompt_prefix=\"<start_of_turn>user\\n\",\n",
        "      prompt_suffix=\"<end_of_turn>\\n<start_of_turn>model\\n\",\n",
        "  )\n",
        "  llm_bundler.create_bundle(config)\n",
        "\n",
        "# Build the MediaPipe task bundle.\n",
        "build_gemma3_1b_it_block_q4()"
      ],
      "metadata": {
        "id": "hlOlionplVpt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
